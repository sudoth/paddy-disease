services:
  triton:
    image: nvcr.io/nvidia/tritonserver:24.08-py3
    container_name: triton_paddy
    command: >
      tritonserver
      --model-repository=/models
      --http-port=8000
      --grpc-port=8001
      --metrics-port=8002
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ./triton/model_repository:/models:ro
    shm_size: "1g"
    # если гпу!
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
